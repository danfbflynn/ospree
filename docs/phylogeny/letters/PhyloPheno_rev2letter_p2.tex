\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1.1in, right=1.1in]{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{xr-hyper}
\usepackage[parfill]{parskip}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{array}
\usepackage{nth}
\usepackage{siunitx}

\externaldocument{..//PhenoPhylo_ms}
% Cross reference line numbers with main text: see https://github.com/temporalecologylab/labgit/wiki/LaTeX#cross-referencing-
\newcommand{\lr}[1]{line~\lineref{#1}}
% \usepackage{hyperref}
\setlength\parindent{0pt}


\begin{document}
Editor and reviewer comments (we provide below the full context of each review) are in \emph{italics}, while our responses are in regular text. 
%\textcolor{blue}{Comments by Lizzie (that need to be deleted) are in blue.}
%\textcolor{teal}{Comments by Nacho (that need to be deleted).}
\\ % and all in-text citations generally cross-reference to the main text.

{\bf Editor's comments:} \\


\emph{At this stage, I would like to ask you to make minor revisions to the work in order to align your paper with the remaining requests raised by R2, and ensure that the paper fits in with our formatting guidelines. We are unlikely to return the paper to reviewers at this point but will assess editorially whether the work is ready to move forward. Please do address the remaining reviewer concerns in a 'response to referees' letter, with changes made tracked within the text.}

We respond to each comment by reviewer \#3 below (we also include the comments from reviewer \#1, though they did not request any changes). In particular, we now include the requested RMSE as an additional metric for prediction accuracy in our cross-validation analysis (with results supporting our previously submitted conclusions) and we clarify how our approach is preferable to others, emphasizing the benefits of fitting a Bayesian phylogenetic mixed model. Also, we have made a minor requested edit to Fig. S9. To review our edits, we provide a version of the manuscript with track-changes as requested, and indicate our changes with line numbers below. We thank the reviewers for their feedback, which has further improved the manuscript.\\

{\bf Reviewer comments:} 

{\bf Reviewer \#1:}\\

\emph{The manuscript ‘Phylogenetic estimates of species-level phenology improve ecological forecasting’ although already novel and of high-impact in the phenological forecasting ecology field; has now been significantly improved through the review process and the written and analytical elements are now of an exceptionally high standard that have broad and important implications in this scientific field.}

\emph{I appreciate the time the authors took to address my comments and the comments of the other reviewers and use this feedback to improve the manuscript. I believe the manuscript now addresses my major concerns by including discussion of how their method generally applies across the globe and also presenting the raw data and raw methods used to model the phenological forecasts. Likewise I appreciate the authors prioritisation and assurance of making their code and data reproducible.}

\emph{Likewise, the authors have taken on board my minor edit suggestions to improve the text and the figures in the manuscript.}\\

We thank the reviewer for their positive feedback as we have done all in our hands to address raised concerns.\\

{\bf Reviewer \#3:}\\ 


\emph{Thanks for the additional analyses that the authors have conducted to support their argument that phylogenetic mixed models performed better than traditional hierarchical mixed models. These additional analyses improved the manuscript though I still have some major comments.}

We agree that the new analyses showing how and when the phylogenetic mixed model outperforms traditional mixed models are both reassuring and have made the manuscript stronger. We thank the reviewer for their previous comments, which lead us to develop these additional analyses. \\


\emph{First, what is the reason to leave one genus out instead of the more traditional way of randomly selecting say 20\% of species to be the testing data? }\\

This is a great question and we realize now that we did not well explain our rationale for this approach. Leave-one-out methods are a standard tool for statistical cross validation, presenting the advantage of being an unbiased estimator of true performance. This is because every data observation is used for validation once, ensuring that all information contributes to the evaluation. These methods are designed to replicate different potential datasets that could have been collected and compare outcomes, with the $k$-fold methods the reviewer mentions specifically assuming each data point is equally likely to be randomly sampled. This assumption is, however, not true in many datasets and we expect especially not true in ecological trait datasets. Our method is thus an extension of  $k$-fold cross validation that incorporates this important non-independence. Further, by leaving-one-clade-out at a time, we can leave out a relatively large number of species (when large genera are held out they can represent nearly 10\% of the dataset) while also accounting for existing phylogenetic structure in the dataset over the validation process.

To explain this we have added a paragraph to the part of the supplement where we explain the method:

\begin{quote}
Traditional cross-validation methods generally leave out a randomly selected proportion of the data (e.g., 20\% in 5-fold cross validation) assuming sampling is completely random (that is, that researchers gathering data would be equally likely to select each observation) and thus ignores potentially important structure in the data that may covary with sampling. In contrast, our method leverages phylogenetic structure, dropping out data in a way that may more accurately reflect differences across differently sampled data. Because of the presence of phylogenetic structure in ecological trait data and the tendency for such datasets to sample across taxa unevenly, we often omit sets of taxa (e.g., clades) sharing similar trait values across different natural sampling regimes. Our results show that the outcomes of this structured omission varies for each model. In HMM, excluding a well-sampled clade would tend to skew parameter estimates more (because it is likely to shift the grand mean towards which all species pool) than in PMM, where the partial pooling co-varies with phylogeny and thus species-level estimates are more stable. 
\end{quote} 


\emph{Second, it is cool to see that the predicted values based on the phylogenetic models are more correlated with the observed values than those based on traditional mixed models. }

Thanks, we were excited to show this also and again thank the reviewer, whose comments lead us to these new analyses. The validation results show that PMM estimates are more stable and provide more robust inferences, reinforcing the main message of the paper.\\


\emph{What are the RMSEs look like for both models? I think RMSEs are more typical when one tries to evaluate predictions. }\\

We agree that RMSE is a standard metric to evaluate model predictions and have now calculated RMSEs and added results to Fig. S9. The prediction error is again lower in PMM (RMSEpmm = 21.6) than in HMM (RMSEhmm = 22.4) confirming previous results. We show these RMSE now alongside the correlation results and related figures for two reasons. First, they are straight-forward and easy to interpret. Second, they help visualize the outcomes of both models: visual inspection of Fig. S9 shows how HMM constrain predicted values to be below 50, while PMM allows for higher estimates.\\

\textcolor{teal}{{\bf Lizzie/JD to edit this whole bit below...}}
\emph{Also, given the marginally improved r values here, when should one take the effort and time to fit phylogenetic mixed models? Was the slightly improved model prediction worth the effort?}\\

% Add in (if not here): We show estimates can change with our method, if you want the best estimates then it is worth it. In addition, it is possible that we might observe larger effects in other datasets, but you can’t know without first comparing the models.
% don't we want the best estimates we can get? And we never know how important it is until we fit the model so we need to fit it. 
% Also, in classic cross-species analysis it is now critical to consider phylogenetic non-independence (de rigeur now) ... so most of trait ecology; we should be doing this in trait ecology

We would argue that increasing r by 0.12 (or 50\% from 0.231 to 0.353) is beyond marginal. We explained in depth when it may be appropriate to fit PMMs both in the ms. (see lines XXX) and in our previous response letter to reviewers. While we believe that allowing for greater interspecific variation is inherently preferable, our model fitting allows the phylogeny to be scaled such that predictions will simply converge on a non-phylogenetic model if phylogeny is not important. 

The reviewer raises again the question of whether the extra effort of fitting the phylogenetic model is worthwhile. Given that our analyses show that estimates can change with our method, then it should be worth for any researcher seeking the best estimates. This is particularly true for ecological forecasting contexts, where it is easy to see how a geographically structured shifts of up to 8 days in phenological estimates for species underrepresented in a dataset (see Fig. S8), can have profound ecological implications. In addition, it is possible that we might observe larger effects in other datasets, but determining so would imply to first comparing the models. We now emphasize more (see lines XX-XX) what is to be gained from making the extra effort of fitting a Bayesian PMM such as ours.\\

\emph{Third, I am not completely sure about the main purpose of this study. Is it an empirical study to investigate different drivers of budburst and predict their future impacts or a method paper to proposal a new model to analyze similar data? If it is an empirical study, then the authors should highlight and discuss the results more. If it is a method paper, how do you facilitate others to use your method? The current manuscript seems to try to do both, which may not be the best choice here.}\\

We would agree with the reviewer that our paper does both: we present a novel statistical approach applied to an empirical dataset that shows some interesting patterns which we interpret. We have adjusted the abstract to make this more clear on lines \lr{whatisyourpaper1} and \lr{whatisyourpaper2}. We are not exactly sure what additional aspects of our empirical results the reviewer would like us to discuss more but would happily do so if any guidance was provided. 

Regarding how we facilitate to use our method, we provide all code and data hoping that interested researchers will start using Bayesian PMMs (see e.g., Appendix XX). We ignore why the referee states that doing both would not be the best choice as such opinion is not justified. We disagree on the basis of several papers published in NCLIM where methodological developments accompany empirical study cases (e.g., Ettinger et al. 2020; XXX). However, we now sign-post the different contributions of the paper more clearly (see lines XX-XX, XX-XX, XX-XX).\\


\emph{Minor comments:}\\
\emph{Can the authors provide a file with track changes? It is so hard to see what have been changed without it.}

We identified all changes to the ms. providing line numbers in our response to reviewers. We realize that a file with track changes would be useful and include it in our current revised manuscript.\\


\emph{Fig. S9 the panel labels are different from the caption.}


Thanks for spotting this mistake! We have now corrected it.\\


\bibliography{phylorefs}
\bibliographystyle{amnat}




\end{document}
